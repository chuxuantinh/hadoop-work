hw {
  db {
    driver = "com.mysql.jdbc.Driver"
    waitTimeout = 21600 # 6 hours
    url = "jdbc:mysql://localhost:3306/HW"
    user = hw
    password = hw
  }

  startup.oozie.indexation.enabled = false
  user = hw
  default.team = panthers
  session.timeout.seconds = 86400

  s3.upload.buffer.size = 33554432

  ldap {
    ssl.enabled = false
  }

  hdfs {
    default.user = hdfs
    default.protocol = http
    default.http.port = 50070
    default.port = 8020
    app.path = "app/panthers_app"
    default.base.path = "/data"
  }

  http {
    incoming {
      request.timeout.ms = 86400000
      content.limit.kbytes = 10240
    }

    outgoing {
      request.timeout.ms = 5000
      connect.timeout.ms = 2000
      idle.timeout.ms = 5000
    }

    upload.content.limit.mbytes = 4000
  }

  platform {
    status.update.interval.seconds = 2
    configs.update.interval.seconds = 3600
  }

  flume {
    cache.update.interval.seconds = -1
  }

  menu {
    disabled = ""
  }

  provision {
    status.update.interval.seconds = -1
    urls : [
      {
        title : "Ansible Tower"
        url: "https://10.223.15.144"
        type: "ANSIBLE_TOWER"
      }
    ]
  }

  externalExecutor = "scala"
  script {
    shellPrefixes = []
  }

  cicd.env.map = {
    dev-int: "dev",
    sandbox: "qa"
  }

  ui.settings = {
    deploymentManagerShowPromoteButtons: true
  }
}

akka {
  daemonic = on
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  actor {
    debug {
      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill et.c.)
      autoreceive = off
    }
  }

  http {
    client {
      connecting-timeout = ${hw.http.outgoing.connect.timeout.ms} ms
    }

    host-connection-pool {

      # The maximum number of parallel connections that a connection pool to a
      # single host endpoint is allowed to establish. Must be greater than zero.
      max-connections = 100

      # The maximum number of open requests accepted into the pool across all
      # materializations of any of its client flows.
      # Protects against (accidentally) overloading a single pool with too many client flow materializations.
      # Note that with N concurrent materializations the max number of open request in the pool
      # will never exceed N * max-connections * pipelining-limit.
      # Must be a power of 2 and > 0!
      max-open-requests = 512


      # The maximum number of times failed requests are attempted again,
      # (if the request can be safely retried) before giving up and returning an error.
      # Set to zero to completely disable request retries.
      max-retries = 0

      client = {

        # The time after which an idle connection will be automatically closed.
        # Set to `infinite` to completely disable idle timeouts.
        idle-timeout = ${hw.http.outgoing.idle.timeout.ms} ms

        # The time period within which the TCP connecting process must be completed.
        connecting-timeout = ${hw.http.outgoing.connect.timeout.ms} ms
      }
    }
  }

  actor.deployment {
    /httpRouter {
      router = round-robin-pool
      nr-of-instances = 2
    }
  }

  aux-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"

    thread-pool-executor = {
      fixed-pool-size = 500
    }

    throughput = 50
  }
}

spray.can {
  server {
    registration-timeout = ${hw.http.incoming.request.timeout.ms} ms
    idle-timeout = ${hw.http.incoming.request.timeout.ms} ms
    request-timeout = ${hw.http.incoming.request.timeout.ms} ms

    parsing {
      max-content-length = ${hw.http.incoming.content.limit.kbytes} kB
    }
  }
}

spray.servlet {
  boot-class = "com.directv.hw.core.web.AppBoot"
  request-timeout = ${hw.http.incoming.request.timeout.ms} ms
  timeout-timeout = ${hw.http.incoming.request.timeout.ms} ms
  max-content-length = ${hw.http.incoming.content.limit.kbytes} kB
}

kamon {

  enabled = false
  config-provider = com.directv.hw.core.kamon.KamonConfigProvider

  trace {
    level = simple-trace
  }

  internal-config {
    akka.loglevel = DEBUG
  }

  metric {

    # Time interval for collecting all metrics and send the snapshots to all subscribed actors.
    tick-interval = 1 seconds

    # Specify if entities that do not match any include/exclude filter should be tracked.
    track-unmatched-entities = yes

    filters {
      akka-actor.includes = [ "**" ]
      akka-router.includes = [ "**" ]
      akka-dispatcher.includes = [ "**" ]
      trace.includes =  [ "**" ]
      trace-segment.includes =  [ "**" ]
      histogram.includes =  [ "**" ]
      min-max-counter.includes =  [ "**" ]
      gauge.includes =  [ "**" ]
      counters.includes =  [ "**" ]
      # For web projects
      http-server.includes = [ "**" ]

      counters.excludes =  [ "" ]
      trace-segment.excludes =  [ "" ]
      histogram.excludes =  [ "" ]
      min-max-counter.excludes =  [ "" ]
      gauge.excludes =  [ "" ]
      http-server.excludes =  [ "" ]
    }
  }

  spm {
    token = ""

    subscriptions {
      akka-actor      = [ "**" ]
      akka-dispatcher = [ "**" ]
      akka-router     = [ "**" ]
      system-metric   = [ "**" ]
      trace           = [ "**" ]
      histogram       = [ "**" ]
      min-max-counter = [ "**" ]
      gauge           = [ "**" ]
      counter         = [ "**" ]
      trace           = [ "**" ]
      trace-segment   = [ "**" ]
      # For web projects
      http-server     = [ "**" ]
    }
  }

  modules {
    kamon-akka.auto-start = yes
    kamon-spm.auto-start = yes
    kamon-system-metrics.auto-start = yes
  }
}
